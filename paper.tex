\documentclass{article}
\usepackage[round]{natbib}
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{geometry}%
\usepackage{color}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{nameref}
\usepackage[right]{lineno}

\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{calc,positioning}

\begin{document}

\linenumbers
\title{What is an Ancestral Recombination Graph?}
% First authors
\author{Author McAuthorface}
% Corresponding

\maketitle

% JK: this is a rough first pass for a slightly different paper. Needs
% substantial revision.
\begin{abstract}
It has recently become possible to infer genetic ancestry in the presence of
recombination at scale for the first time, enabling many
downstream applications in population and statistical genetics.
Such recombinant genetic ancestry is usually
referred to as an Ancestral Recombination Graph, or ARG.
% Note: slight repetition here with first sentence.
There are now multiple methods that can infer ARGs at a practical scale
and it is therefore vital that these methods can be systematically evaluated
and compared to determine their strengths and weaknesses.
Unfortunately, however, there is substantial confusion over
terminology and little agreement on shared standards for data interchange,
significantly hampering progress in this important area.
This confusion and lack of agreement on basic issues such as what does and
does not constitute an ARG is partly attributable to the historical
development of the term.
Originally rigorously defined as
a stochastic process (the coalescent with recombination) together with its
representation, an ARG is now understood to refer to any
concrete realisation of a recombinant genetic ancestry.
We show that the standard description of an ARG
in terms of common ancestor and recombination events
(inherited from the original stochastic process definition) is
ambiguous, fundamentally limited and an unsuitable basis for data interchange.
We provide a simple alternative definition of an ARG in which a node
corresponds to one of an individual's monoploid genomes, an edge
% jk-note: this is clumsy
defines a genetic ancestry relationship between two nodes,
and annotations on those edges define the precise patterns of
ancestry in intervals along the genome.
We use this framework to define some classifications and metrics of ARG nodes,
and discuss the properties of some recently developed inference methods.
% jk-note: this ends a bit flat - we should say other stuff like we clarify
% how the ARG relates the pedigree, etc, etc. Let's go through again when
% we have a reasonable draft.
\end{abstract}

\textbf{Keywords:} Ancestral Recombination Graphs

\section*{Introduction}
Inferring the genetic relationships between sampled genomes in the form of an
evolutionary tree is a necessary prerequisite for many analyses of species \citep{rannala2003genetics}, non (or
rarely) recombining sections of DNA \citep{cann1987mitochondrial,underhill2001annalsofhumangenetics} and viruses \citep{grenfell2004science}. The tree captures what is known and
knowable about the genetic ancestry of the sample in an elegant and concise
way, by postulating a set of nodes that are ancestors common to the samples,
and the ancestral relationships between those nodes. A rich literature exists
on analysing the mathematical properties of these trees [cite?], and numerous
methods exist to infer them~\citep{felsenstein2004inferring}. The situation is
quite different in recombining organisms, where the analysis of genetic data
depends on summaries ultimately derived from the
trees~\citep{tajima1983evolutionary,tavare1984line,fu1995statistical}
such as allele frequencies and site frequency
spectra~\citep{achaz2009frequency,ralph2020efficiently},
patterns of linkage disequilibrium~\citep{mcvean2002genealogical}, and principal
components~\citep{mcvean2009genealogical}. Genealogical analysis is equally
fundamental to the study of recombining organisms, but profound technical
difficulties have made the routine inference of recombinant genealogies
impossible, and therefore tree-based analysis methods have been largely
unavailable to the fields of population and statistical genetics.

Recent breakthroughs in large-scale inference
methods~\citep{rasmussen2014genome,kelleher2019inferring,speidel2019method,
schaefer2021ancestral,wohns2021unified}
and data representation~\citep{kelleher2016efficient}
have raised the genuine prospect of genealogical analysis becoming a standard part
of the population and statistical genetics toolkit.
Applications using these inferred ancestries as input have
begun to appear~\citep{osmond2021estimating} and many more are sure to
follow~\citep{harris2019database}. This vibrant research area, however,
has a significant difficulty, which, if not addressed, will substantially hamper
progress. We currently lack a well-defined data-model and shared terminology
to discuss recombining genealogies,
leading to basic errors in statements about what different inference methods
produce as output (see the XXX section below), as well as presenting serious
difficulties in either comparing the outputs of different methods or
(from a users perspective) using multiple inference methods in an analysis.

In this paper we [turn into proper round up paragraph later]
\begin{enumerate}
\item Discuss the classical Ancestral Recombination Graph formulation,
and show how it derives from (and is limited by) the coalescent with
recombination.
\item Suggest a definition of ARGs that is free of these limitations,
where the formulation is derived from a (normally diploid) pedigree rather than from
coalescent approximations, and where the genetic
material which is transmitted is described by annotations attached to
edges rather than nodes.
\item Suggest a classification of ARG nodes that helps us to understand
the properties of the structures inferred by different methods, and illustrate
with some examples.
\end{enumerate}

\section*{Ancestral graphs and stochastic processes}
The coalescent~\citep{kingman1982coalescent,kingman1982genealogy,
hudson1983testing, tajima1983evolutionary}
provides the theoretical underpinning for much of contemporary population genetics.
Hudson extended the coalescent to include
recombination~\citep{hudson1983properties,hudson1990gene}
and the model has proved to be an especially powerful basis for
simulations~\citep{hudson2002generating,baumdicker2021efficient}.
Hudson's algorithm is concerned with simulating the process of
the coalescent with recombination as efficiently as possible, and operates by
tracking segments of ancestral material carried by ancestors as
we go backwards in time. The details of Hudson's algorithm and how the
results of the ancestral process are represented are critical to later
discussions, and so we will explain the process in some detail.
See~\cite{kelleher2016efficient} for a more detailed
description of the algorithm, and how it may be implemented efficiently.

\begin{figure}
\caption{\label{fig-sim-statespace} An illustration of the state space
of Hudson's algorithm (A) and the big ARG (B) at a given instant. The
state maintained by Hudson's algorithm is considerably more complicated,
but is guaranteed to only simulate events corresponding to genetic
ancestors of the sample.
}
\end{figure}


Hudson's algorithm operates by tracking the state of a set of ancestral
lineages as we go backwards in time.
% jk-note unclear here about the best choice of notation. We could have
% lists of tuples (l, r, a), or a tuple of vectors (l_j, r_j, a_j),
% of a tuple (I_j, a_j), where I is a vector of intervals. Going to
% stick with the original tuple notation for now.
%
% Each lineage consists of a set of
% equal-length vectors $\mathbf{l}$, $\mathbf{r}$ and $\mathbf{a}$
% such that for each $1 \leq j \leq \left| l \right|$,
% $[\mathbf{l}_j, \mathbf{r}_j)$ is a half-closed genomic interval and
% $\mathbf{a}_j$ is an integer
% tracking the number of samples the lineage is ancestral to over the interval.
% Assume that the intervals are sorted from left-to-right so that $\mathbf{l}_1$
% and $\mathbf{r}_{|\mathbf{r}|}$ are the left- and right-most positions
% covered by the lineage, respectively.
Each lineage consists of a list of
disjoint ancestry segments $(\ell, r, a)$, where
$[\ell, r)$ is a half-closed genomic interval and $a$ is an integer
tracking the number of samples to which the lineage is ancestral over that interval.
(We also usually track the tree node associated with each segment, but
that is not important for our purposes here so we omit it.)
If we have $n$ samples and genome of length $m$, the process begins with $n$ lineages
of the form $[(0, m, 1)]$. The process then works backwards in time from
the present day as a series of random common ancestor or recombination events.
Recombination events occur at a rate determined by the amount of ancestral material and
the way in which it is distributed along a chromosome.
Let $L$ be the set of ancestors at a given time $t$. Recombination events
happen at rate $\rho \nu / (m - 1)$ where
\[
\nu = \sum_{x \in L}\left( \max_{(\ell, r, a) \in x}r
    - \min_{(\ell, r, a) \in x}\ell - 1 \right)
\]
is the number of available `links' that may be broken. Thus, the rate of
recombination is determined by the left- and right-most extent of the
ancestral material carried by each lineage. At a recombination
event we choose one of these links uniformly and break it. Given a lineage
$x = [(\ell_j, r_j, a_j)]$ and a breakpoint $k$, we have two lineages
$x_1$ and $x_2$ such that FILL IN DETAILS

When $k = |L|$ lineages are present, common ancestor events
occur at rate $\binom{k}{2}$. In a common ancestor event, two lineages
are chosen uniformly at random and their ancestry segments merged.
If we have overlapping intervals of ancestry from the two lineages,
say, $(\ell, r, a_1)$ and $(\ell, r, a_2)$, a
\emph{coalescence} occurs and ancestor represented by the current event
will be present as a node (at least) in the marginal trees covering
the interval $[\ell, r)$. The result of this coalescence is a segment
$(\ell, r, a_1 + a_2)$, and if $a_1 + a_2 < n$ it is included in the
ancestry for the new lineage. Otherwise, if $a_1 + a_2 = n$ we know that
we have found the most recent common ancestor of all samples in
the interval $[\ell, r)$ and so we do not need to simulate its history any further.
Nonoverlapping intervals of ancestry from the two lineages are included
in the resulting lineage without changes. Eventually, as the process continues,
we find resultant lineages in which all segments have fully coalescenced,
and so the number of extant lineages gradually dwindles down to zero.

% NOTE: it would probably be helpful to have a drawing of the ARG process
% in-progress here to help us talk about it. What is the full state
% that is remembered as we go through the process? It does help to think
% about the process as choosing edges to merge, not nodes.
The Ancestral Recombination Graph (ARG) was later introduced by Griffiths~\citep{griffiths1991two,ethier1990two,griffiths1996ancestral,
griffiths1997ancestral}, and is a
closely related and complementary treatment of the same process. Where
Hudson's algorithm is focused on the efficient \emph{simulation} of the
process, the original ARG literature is focused on mathematical
results about the stochastic process. Griffiths and colleagues formulated the
ARG as a branching-coalescing stochastic process. We imagine a graph in which
each edge corresponds to an extant lineage and nodes are events in the
process (the initial $n$ leaf nodes are ``sampling'' events). As in Hudson's
algorithm, common ancestor events occur at rate $\binom{k}{2}$ when there
are $k$ lineages present. We choose two lineages (edges) uniformly, and merge them
into a common ancestor lineage. Recombination events are different
to Hudson's algorithm, however. Rather than occuring at a rate which depends on
the specific distribution of ancestral material among lineages, the
rate of recombination is simply $k \rho (m - 1)$ in the original ARG
formulation. At a recombination event we choose a lineage (edge) uniformly, and a
breakpoint $0 < x < L$ uniformly on its genome. We terminate the edge at a
node, record the breakpoint and start two new edges from this node. The process
then continues until there is only one lineage left (the Grand Most Recent
Common Ancestor, GMRCA), which is guaranteed to
happen in finite time because of the linear vs quadratic rates of branching
and coalescing. The graph structure and breakpoints associated with
recombination nodes provides sufficient information to later recover the marginal
trees (see the next section for more details and discussion on this point).

The state-space of this process is much simpler than Hudson's algorithm, which
greatly facilitates mathematical reasoning. This simplicity comes at a
substantial cost, however, if we wish to use it as a practical means of
simulating recombinant ancestry. The ARG is vast, and any realisation
for even moderate levels of recombination is far too large to be of practical
use. The number of events in the ARG back to the GMRCA
is $O(e^\rho)$~\citep{griffiths1997ancestral}, whereas the number
of events required to simulate the complete ancestry of a sample
in Hudson's algorithm is
$O(\rho^2)$~\citep{hein2004gene,baumdicker2021efficient}.
This disparity in the number of events in the two formulations is
because the vast majority of the events that occur in the ARG do
not affect the genetic ancestry of the sample in any way. Recombination
events occur outside of ancestral material do not have any bearing
on the ancestry of the sample, and so the structure is hugely redundant.
As~\cite{wiuf1999recombination} note,
``an `ancestral' sequence in the birth and death process
need not have any genetic material in common with a
sequence descended from it.''

% Full quote:
% This process simplifies mathematics on the account that the notion of an
% ancestor will have a less restrictive meaning than usual:
% An ``ancestral'' sequence in the birth and death process
% need not have any genetic material in common with a
% sequence descended from it.

This original formulation which does not keep track of the distribution
of ancestral material among lineages is the so-called ``Big'' ARG.
The ``little'' ARG is the subset of the Big ARG traversed by
Hudson's algorithm (\citet{wiuf1999recombination} refer to this as HUD).
[Review of other work done on the ARG stochastic process here.
Discuss \citet{wiuf1999ancestry}. Wiuf and Hein's classical pair of papers
clarified the relationship between Hudson's algorithm and the ARG]

Similar graph encodings have also been introduced to model other mechanisms
which change the local ancestral tree along the genome, such as gene
conversion~\citep{wiuf2000coalescent} and
horizontal gene transfer~\citep{baumdicker2014infinitely}.
[Mention the
SMC~\citep{mcvean2005approximating,marjoram2006fast}
and ClonalFrame~\citep{didelot2007inference} approximations somewhere.]

The idea of using branching-coalescing stochastic processes
to model genetic ancestral trees has also found application in natural selection.
The Ancestral Selection Graph (ASG)~\citep{krone1997ancestral,neuhauser1997genealogy}
uses dynamics identical to the ``big" ARG to simulate an ensemble of correlated
potential ancestral trees. Weak genic selection is incorporated by sampling a
true ancestry from the ensemble in a non-uniform way.
There is no ASG analogue of the more computationally tractable ``little" ARG,
though some gains in tractability can be made by considering typed
lineages~\citep{etheridge2009coalescent} or by leveraging perfect simulation
techniques when recurrent mutation is present\citep{fearnhead2001perfect}.
Extensions of the ASG have been developed to frequency-dependent
selection~\citep{neuhauser1999ancestral, gonzalezcasanova2018duality},
unlinked chromosomes~\citep{fearnhead2003ancestral}, recombining
loci whereupon branching is due to both selection and
recombination~\citep{donnelly1999genealogical}, and high fecundity
reproduction~\citep{gonzalezcasanova2018duality, koskela2019robust}.


\begin{figure}
\centering
\begin{tikzpicture}[xscale=0.8, yscale=0.3, node distance=2mm and 20mm]
\tikzset{greynode/.style={circle,fill,inner sep=1},
nodelabel/.style={font=\footnotesize}}

\node (s0) [greynode] {};
\node (s1) [right=of s0,greynode] {};
\node (s2) [right=of s1,greynode] {};
\node [greynode] (s3) at ($(s1) + (0,1)$) {};
\node [greynode] (s4) at ($0.5*(s1) + 0.5*(s2) + (0,2)$) {};
\node [greynode] (s5) at ($0.5*(s0) + 0.5*(s1) + (0,3)$) {};
\node [greynode] (s6) at ($(s1) + (0,5)$) {};

\node [nodelabel,anchor=north west] at ($(s3) + (0,0)$) {$x = 0.3$};
\foreach \u/\lab in {s0/0, s1/1, s2/2} \node[nodelabel,anchor=north] at (\u) {\lab};
\foreach \u/\lab in {s4/4} \node[nodelabel,anchor=south west] at (\u) {\lab};
\foreach \u/\lab in {s5/5} \node[nodelabel,anchor=south east] at (\u) {\lab};
\foreach \u/\lab in {s3/3, s6/6} \node[nodelabel,anchor=south] at (\u) {\lab};

%% Edges
\draw (s1) -- (s3);
\foreach \child/\parent in {s6/s5, s6/s4, s5/s0, s4/s2}
	\draw (\child) -| (\parent);

\draw (s5) -| ($0.5*(s3) + 0.5*(s3-|s5)$) |- (s3);
\draw (s4) -| ($0.5*(s3) + 0.5*(s3-|s4)$) |- (s3);

\end{tikzpicture}
\caption{\label{fig-arg} \textcolor{red}{A picture of a standard ARG for us
to work with for now. We should replace it with something better at some
point.}
 }
\end{figure}

\section*{The ARG as a data structure}
Early work on ARG inference focused on the problem of
of inferring parameters of the
stochastic process, where the ancestry is regarded as a
latent parameter to be averaged out
\citep[e.g.][]{griffiths1996ancestral,kuhner2000maximum, nielsen2000estimation,
fearnhead2001estimating}. These methods met with limited success
due to the overwhelmingly large and
horribly structured [Jere: what's the right way to say this?]
state space of ARGs. The focus subsequently shifted to
the more tractable---but still
NP-hard~\citep{wang2001perfect}---problem of computing
the minimum number of recombinations required
to explain an input dataset [CITATIONS], and inferring the corresponding
ARG realisations~\citep{song2003parsimonious,song2005efficient,lyngso2005minimum}.
The distinction between the ARG as a stochastic process
and the ARG as a data structure became less clear at this point,
with \citet{minichiello2006mapping} explicitly arguing for
the data structure interpretation.

The Griffiths ARG encoding is an elegant and mathematically
economical description of the coalescent with recombination on a single
chromosome, but it has significant drawbacks as the basis for a general purpose
storage format.

[Not sure how to make a narrative out of this, but here are the
basic problems:]
\begin{enumerate}
\item The encoding is hamstrung by the interpretation of
nodes as \emph{events} of a particular type. We can either
have common ancestor or recombination events. This is true
in the coalescent, but is not in a more general Wright-Fisher
model.
%% Note - I'm not sure this is true, if the nodes are *events* rather than
%% genomes, unless you want events (rather than genomes) to occur at
%% integer generations.
\item We can also only have one crossover per recombination event.
This is unrealistic because things like gene conversion are common. It also
means we can't have back and forth recombination between
two parents, or represent the transmission of multiple chromosomes.
\item The passage of ancestral material along edges requires
extrinsic rules which must be shared by all implementations.
These rules can be quite restrictive when generating, and
in practise some have found it simpler to add explicit
annotations to the edges.
\item Generating local trees is inefficient, essentially requiring
$O(n T)$ time (or more?)
\end{enumerate}


[WIP jk]

This ambiguity can be dealt with in a number of ways. In the original
treatments, the direction in which ancestral material travels at
a breakpoint is handled by a simple rule, sites to the left of the
breakpoint go to the left parent and sites to right go to the
right parent. [Illustrate the problem with simulating an ARG
like this]. ~\cite{ignatieva2021kwarg} resolve this problem by
associating a label ``P'' (prefix) or ``S'' (suffix) with the
outbound edges of a recombination event. This is the same approach
as used in beagle~\citep{lyngso2005minimum}.

\section*{Anchoring the ARG to the pedigree}

The limitations of the Griffiths ARG encoding are a direct consequence
of its derivation from the coalescent with recombination stochastic process.
Indeed, these limitations arise because it is such a \emph{good}
mathematical encoding, providing sufficient information to
define the realised ancestry under the assumptions of the model,
and no more.

We describe here an alternative formulation in which the
concepts are based on the diploid pedigree, which
is free from the limitations discussed in the previous section.
\cite{mathieson2020ancestry} and others 
\citep{wakeley2012genetics,gusfield2014recombinatorics,speed2015naturereviewsgenetics}
have also suggested this approach, although without explicitly defining
the roles of nodes and the details of how ancestry
flows along edges.

\begin{figure}
\vspace{5em}
\caption{\label{fig-pedigree-and-arg}
The (unannotated) ARG is closely related to the pedigree
graph, where each ``individual'' node is replace by two ``genome''
nodes, and edges between parents and children are only included if there is
transfer of ancestral material. We usually only include nodes in an ARG
that affect our sample, so that, for example, individuals that are ancestral
to only one sampled genome are not included.}
\end{figure}

[Here are the basic components of the model]

\begin{enumerate}
\item A node corresponds to one of a diploid individual's two genomes.
This is linked explicitly to the pedigree, and is fully general.
Nodes \emph{may} be labelled with certain flags describing their properties,
but it's not necessary.
\item An edge between two nodes defines a genetic inheritance relationship
between two nodes. Edges are annotated with information about the
passage of ancestral materal. There are different forms these annotations
can take. For example, we can follow the approach taken in the Griffiths
ARG and just annotate recombinant edges with the interval $[0, x)$ or $[x, L)$,
depending on side of the breakpoint the edge is on. In the most
general case, we annotate each edge with the full set of disjoint
intervals of ancestral material that flows along that edge.
\citep{mcgill2013graphml} also proposed full ancestral material being
associated with each edge.
\end{enumerate}

\begin{figure}
\vspace{5em}
% NB: not clear if this figure is better off on its own or should be
% merged with the pedigree one.
\caption{\label{fig-arg-annotations}
The specific path through the ARG for a given genome position (and therefore
the local genealogical tree) cannot be determined without annotations
to the graph. (A) The classical Griffiths approach annotates recombination
nodes with the corresponding breakpoint. (B) We can equivalently
annotate the \emph{edges} with the genomic intervals carrying ancestral
material. Edge annotations are somewhat more general, and have
significant computational advantages.}
\end{figure}

\begin{figure}
\centering
\vspace{5em}
\includegraphics[width=\linewidth]{illustrations/ARG_recomb_node_deletion}
\caption{\label{fig-recombination-nodes}
Recombination nodes are optional in an edge-annotated ARG. (a) A simulated
ARG including all recombination nodes (red). (b) The same ARG having removed both
recombination nodes and nodes of common ancestors in which no genetic
lineages coalesce (cyan). (c) The sequence of trees corresponding to (b): if we were
to plot out the trees corresponding to (a) we would have more trees, but the topologies
and MRCA nodes at each genomic location would be identical. For clarity, nodes are
plotted by ranked time rather than true time on the y-axis.
% HYW: note that we also truncate nodes for portions in which they are unary,
% so that nodes which are coalescent nodes over only some of their length
% (blue-green) become coalescent nodes over their entire length (green). This
% can, however, throw away some useful information, for example node 4 has
% only a single parent edge in (a), but 2 parent edges (one of which appears
% to bypass node 11) in (b). In fact, this edge should go through node 11, but
% there may be no way to detect this from real data.
}
\end{figure}

\section*{Relationship with other types of phylogenetic networks}

The term \emph{phylogenetic network} has been broadly accepted to describe the most general
type of genealogical graph, defined by \citet{huson2010phylogenetic} as ``any graph used to
represent evolutionary relationships (either abstractly or explicitly) between a set of taxa
that labels some of its nodes" (although more narrow and context-dependent definitions can also
be found in the literature). Explicit rooted phylogenetic networks are rooted DAGs,
with leaves labelled by the taxa (species, groups, individuals or sequences), and nodes classed
as either reticulation nodes (with in-degree $\geq 2$) or tree nodes.

Some sub-classes of explicit rooted phylogenetic networks are defined based on the evolutionary event that the
reticulation nodes capture \citep{huson2010phylogenetic}. In \emph{reassortment} networks, used to
model the evolution of viruses, reticulation nodes correspond to exchange of
(non-recombining) genomic segments between viral particles inside a co-infected host.
In \emph{hybridisation} networks, reticulation nodes correspond to genetic material from two
different species combining to create a hybrid. In \emph{recombination} networks, the reticulation nodes
correspond to recombination events (with leaves labelled by sequences and edges labelled by mutations).
The ARG is a subtype of a recombination network under this definition (restricting to one crossover
recombination per reticulation node).
% AI: Not sure about this - do ARGs also encompass multiple crossover recombinations and gene
% conversion (seems implied above)?

Sub-classes of explicit rooted phylogenetic networks can also be defined by restrictions on their topology.
In \emph{binary networks}, all internal nodes must have either in-degree two and out-degree one or in-degree
one and out-degree two (with the root having out-degree two and the leaves in-degree 1) \citep{steel2016phylogeny}.
In \emph{tree-child networks}, every internal node must have at least one child that is a tree node;
\emph{tree-sibling networks} are constrained by every reticulation node having at least one sibling that is a
tree node \citep{cardona2008extended}. Further, sub-types can be defined by restrictions on
the topology of subgraphs that contain the reticulation loops (cycles containing a reticulation node).
A reticulation loop that does not share any node with another reticulation loop is termed a gall
\citep[][p.\ 237]{gusfield2014recombinatorics}; a reticulation network containing only galls is
called a \emph{galled tree}. Reticulation loops that are joined together in a reticulation network
by at least one shared node form a blob; a \emph{level-$k$ network} is one where the maximal number
of reticulation loops inside a blob is $k$ \citep{choy2005computing}. Galled trees are level-1 networks,
and the tractability afforded by their relatively simple structure results in a number of attractive
combinatorial properties \citep{wang2001perfect, gusfield2004optimal}. We consider the general case of
an ARG, without applying any of these restrictions.
% AI: have not mentioned any implicit or un-rooted networks (split, median, haplotype, reticulograms, etc).
% Also: are we not defining ARGs as "binary networks"? Might be good to specify this here.

\section*{ARG inference methods}
Using the terminology developed here to classify ARGs, we now review methods developed
to explicitly \emph{infer} ARGs from sequencing data. 

\subsection*{Parsimony and heuristics}
The problem of reconstructing ARGs for samples of recombining sequences has been of significant
 interest essentially since the ARG was first defined. Early methods focussed on finding the most 
 \emph{parsimonious} ARGs, i.e.\ those minimising the number of posited recombination events in 
 the history of the sample \citep{hein1990reconstructing}. Two main approaches to explicitly reconstructing 
 parsimonious ARGs then emerged: building the ARG either \emph{backwards-in-time} or \emph{along-the-genome}. 
 The former approach, introduced by \citet{lyngso2005minimum} and implemented in the program 
 \texttt{Beagle}, starts with the input data matrix and reduces it through row and column operations 
 corresponding to coalescence, mutation and recombination events, until the empty matrix is reached; 
 the corresponding sequence of events can then be used to construct an ARG from the bottom up. The 
 latter approach, introduced by \citet{song2003parsimonious} and first implemented in the program 
 \texttt{RecMinPath} \citep{song2005constructing}, starts with an initial local tree at some starting 
 position, and refines the tree moving left and right along the genome, changing the topology of the 
 tree when necessary through \emph{subtree prune and regraft} (SPR) operations (corresponding to 
 recombination events). 

Reconstructing a most parsimonious ARG for a given data set is NP-hard \citep{wang2001perfect}, 
and in order to allow for larger input datasets, subsequent parsimony-based methods have resorted 
to various relaxations and heuristics. Those utilising variants of the backwards-in-time approach 
include the programs \texttt{SHRUB} \citep{song2005efficient}, \texttt{GAMARG} \citep{thao2019hybrid}, 
and \texttt{KwARG} \citep{ignatieva2021kwarg}, and the method described by \citet{wu2008association}. 
Tools constructing genealogies along-the-genome include \texttt{RecPars} \citep{hein1993heuristic}, 
\texttt{RENT} \citep{wu2011new} and its sequel \texttt{RENT+} \citep{mirzaei2017rent}. Other approaches 
include \texttt{TARGet} \citep{camara2016inference}, which implements a method based on topological 
data analysis. The scalability of these methods varies substantially based on the heuristics in use 
and the underlying data structure, with the most efficient tools limited to analysing hundreds of 
sequences. 

A number of heuristic methods have also been developed which do not (explicitly) focus on reconstructing 
the most parsimonious ARGs, but rather aim for computational efficiency while seeking to achieve good 
accuracy of the inferred genealogies. Such methods include \texttt{Margarita} \citep{minichiello2006mapping} 
and the approach of \citet{parida2008estimating}; the more recently developed \texttt{Relate} 
\citep{speidel2019method} and \texttt{tsinfer} \citep{kelleher2019inferring} use a combination of both 
backwards-in-time and along-the-genome approaches, and can be practically used on megabase scale data 
and tens to hundreds of thousands of samples under human-like parameters.
Another class of methods has recently emerged that use a \emph{threading} approach, constructing ARGs 
by adding sequences to the graph one-by-one. Originally introduced by \citet{rasmussen2014genome} as a 
way of generating MCMC proposals for ARGs, a similar idea was leveraged in \texttt{ARGneedle} 
\citep{zhang2021biobank} to reconstruct ARGs for biobank-scale data.
% AI: just to note, these papers do not all agree in their definition of an ARG.
% I guess this is one of the points of the present paper, but should we mention this fact?
% Should we also mention that most of these infer just the topology, but some also the times?

\subsection*{Model-based methods}
An alternative approach is to treat the ancestry as a latent parameter to be averaged out
by Monte Carlo methods, based either on importance sampling
\citep{griffiths1996ancestral, fearnhead2001estimating, jenkins2011inference}
or MCMC \citep{kuhner2000maximum, nielsen2000estimation, wang2008bayesian, fallon2013acg}.
These methods invariably operated on a representation of the ``little ARG", typically
generated until the grand MRCA. They are extremely computationally expensive,
and applicable to at most hundreds of samples consisting of tens of kilobases with
human-like parameters.

Due to the computational expense of sampling ``little ARGs", state-of-the-art
Monte Carlo methods typically rely on cheaper, approximate models.
The most prominent examples are \texttt{ARGWeaver} \citep{rasmussen2014genome}
and \texttt{Arbores} \citep{heine2018bridging}. The former also utilises a time
discretisation approximation, but scales to dozens of mammal-like genomes.
The \texttt{ClonalOrigin} method \citep{didelot2010inference,
medina2020speeding} uses the \texttt{ClonalFrame}
model and data structure, though it is not as scalable as \texttt{ARGWeaver}.

A very recent MCMC method called \texttt{ARGInfer} has resulted in the first
improvement in the scalability of Monte Carlo methods for the exact ARG model
by making use of the tree sequence representation of marginal trees, suitably enriched
to facilitate the computations needed for MCMC sampling \citep{mahmoudi2021inference}.
While not competitive with the scalability of \texttt{ARGWeaver}, \texttt{ARGInfer} extends
the feasible range of sequence lengths by at least an order of magnitude when
compared with methods which sample realisations of the exact ``little ARG".


\begin{figure}
\vspace{5em}
\caption{\label{fig-inferred-args}
ARGs inferred by different methods from a small dataset. Notes about these
ARGs.
}
\end{figure}

\section*{Discussion}

Discussion points (may or may not be used, certainly would be heavily revised)

\begin{itemize}
\item Terminology based around an inferred ancestry being a ``true''
ARG is impoverished and misleading. The only reasonable interpretation
of a ARG being true or not is whether it's a sample from the coalescent.
Most inference methods (and all that scale well) do not. In any case,
the coalescent is just one model, and a highly idealised one. It is
exceedingly unlikely that the actual ARG describing the ancestry
of any biological population is ``true'' ARG in this sense. The terminology is
impoverished because it reduces the possibilities available
to something either being an ARG or not, whereas there are many different
interesting properties of these structures that we should be able to
discuss and compare.
\item tskit can represent any ARG.
\item Interchange is a real problem, and if we're going to realise the
potential for ARGs in genomics we must solve it. Efforts to standardise
phylogenetic networks haven't done very well (~\citep{cardona2008extended}
has 89 citations). The only real effort to standardise an ARG format
based on extending GraphML~\citep{mcgill2013graphml} has 11 citations.
GraphML~\citep{mcgill2013graphml} has some of the same key ideas
as tskit though, in particular allowing the annotation of ``live sites''
to an edge as a set of disjoint intervals that hold ancestral material.

\item We should discuss other types of ancestry network also.
The crucial distiction between an ARG and a phylogenetic network (I think?) is
the presence of position information specifying the location of
recombination events (and by extension, the complete passage
of ancestral material the graph). Without position information
we know that a particular node was an ancestor somewhere along the
sequence, but not exactly where.

\end{itemize}

\bibliographystyle{plainnat}
\bibliography{paper}

\end{document}
